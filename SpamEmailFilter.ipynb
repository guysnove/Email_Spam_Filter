{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6422715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and modules\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from MultinomialNB import MultinomialNB_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fadab786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "test_file_path = 'test-mails'\n",
    "train_file_path = 'train/train-mails'\n",
    "print(len(test_file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7913edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordMap = {}\n",
    "commonMap = []\n",
    "\n",
    "most_common_word = 3000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f511fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid 0 terms in features\n",
    "smooth_alpha = 1.0\n",
    "\n",
    "class_num = 2  # we have only two classes: ham and spam\n",
    "class_log_prior = [0.0, 0.0]  # probability for two classes\n",
    "feature_log_prob = np.zeros((class_num, most_common_word))  # feature parameterized probability\n",
    "SPAM = 1  # spam class label\n",
    "HAM = 0  # ham class label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9814d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read file names in the specific file path\n",
    "def read_file_names(file_path):\n",
    "    return os.listdir(file_path)\n",
    "\n",
    "# read in the specific file\n",
    "def read_file(file):\n",
    "    content = ''\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                content += line\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f838680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_word(words):\n",
    "    for word in words:\n",
    "        if not word[0].isalpha():\n",
    "            continue\n",
    "        if not (word in wordMap.keys()):\n",
    "            wordMap[word] = 1\n",
    "        else:\n",
    "            count = wordMap[word]\n",
    "            wordMap[word] = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "846f6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the word in one file\n",
    "def count_word(words, singleWordMap={}):\n",
    "    for word in words:\n",
    "        if not word[0].isalpha():\n",
    "            continue\n",
    "        if not (word in singleWordMap.keys()):\n",
    "            singleWordMap[word] = 1\n",
    "        else:\n",
    "            count = singleWordMap[word]\n",
    "            singleWordMap[word] = count + 1\n",
    "\n",
    "        # find the most common words in files store in commonMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e20bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common():\n",
    "    # sort the wordMap\n",
    "    sort_wordMap = {k: v for k, v in sorted(wordMap.items(), key=lambda x: x[1], reverse=True)}\n",
    "\n",
    "    # add the most common words into commonMap\n",
    "    index = 0\n",
    "    for key in sort_wordMap.keys():\n",
    "        if index < most_common_word:\n",
    "            commonMap.append(key)\n",
    "        else:\n",
    "            break\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6b4aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate features according to commonMap\n",
    "def generate_feature(features, path, files):\n",
    "    singleWordMap = {}\n",
    "    file_index = 0\n",
    "    for file in files:\n",
    "        singleWordMap = {}\n",
    "        content = read_file(path + '/' + file)\n",
    "        content.replace(\"\\n\", \"\")\n",
    "        contents = content.split(\" \")\n",
    "        # print(contents)\n",
    "        count_word(contents, singleWordMap)\n",
    "\n",
    "        for key1 in singleWordMap.keys():\n",
    "            common_index = 0\n",
    "            for key2 in commonMap:\n",
    "                if key1 == key2:\n",
    "                    features[file_index][common_index] = singleWordMap[key1]\n",
    "                common_index += 1\n",
    "        file_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38813e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum of most_common can be:  18708\n"
     ]
    }
   ],
   "source": [
    "# construct dictionary\n",
    "files = read_file_names(train_file_path)\n",
    "\n",
    "for i in range(len(files)):\n",
    "    content = read_file(train_file_path + '/' + files[i])\n",
    "    content.replace(\"\\n\", \"\")\n",
    "    contents = content.split(\" \")\n",
    "    count_total_word(contents)\n",
    "\n",
    "print(\"The maximum of most_common can be: \", len(wordMap))\n",
    "\n",
    "most_common()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b54469a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702\n"
     ]
    }
   ],
   "source": [
    "# construct model\n",
    "# training feature matrix\n",
    "train_features = np.zeros((len(files), len(commonMap)))\n",
    "generate_feature(train_features, train_file_path, files)\n",
    "print(len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8609f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702\n"
     ]
    }
   ],
   "source": [
    "# training labels\n",
    "train_labels = np.zeros(len(files))\n",
    "for i in range(len(files) // 2, len(files)):\n",
    "    train_labels[i] = 1\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d4ee7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify model\n",
    "# load test data\n",
    "files_test = read_file_names(test_file_path)\n",
    "# testing feature matrix\n",
    "test_features = np.zeros((len(files), len(commonMap)))\n",
    "generate_feature(test_features, test_file_path, files_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b75fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing labels\n",
    "test_labels = np.zeros(len(files_test))\n",
    "for i in range(len(files_test) // 2, len(files_test)):\n",
    "    test_labels[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3e1d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes start\n",
    "# print(train_labels)\n",
    "# train model\n",
    "MultinomialNB = MultinomialNB_class()\n",
    "MultinomialNB.MultinomialNB(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41781a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes:  0.9615384615384616\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "classes = MultinomialNB.MultinomialNB_predict(test_features)\n",
    "error = 0\n",
    "for i in range(len(files_test)):\n",
    "    if test_labels[i] == classes[i]:\n",
    "        error += 1\n",
    "print(\"Multinomial Naive Bayes: \", float(error) / float(len(test_labels)))\n",
    "# Multinomial Naive Bayes end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f15693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "              \n",
    "#MultinomialNB.MultinomialNB_predict(new_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520daac4",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c38ba90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a33da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd665db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4811db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a802faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5990dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
